{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a027e6b5-33a4-4fc4-ab59-e637834d914f",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b531056d-5081-42c3-b657-e4727b85a7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import hashlib\n",
    "import io\n",
    "import json\n",
    "import pickle\n",
    "import requests\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "from scipy.stats import ks_2samp\n",
    "from plotchecker import LinePlotChecker, ScatterPlotChecker, BarPlotChecker\n",
    "import seaborn as sns\n",
    "import requests\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler,OrdinalEncoder\n",
    "from sklearn.metrics import precision_score, recall_score, precision_recall_curve\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from lightgbm import LGBMClassifier, LGBMRegressor \n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from pmdarima import auto_arima"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290b8fa0",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b070ed40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grouped_weighted_mape_bias(df, group_cols, y_true_col, y_pred_col, quantity_col):\n",
    "    # Function to compute weighted MAPE & BIAS\n",
    "    grouped = df.groupby(group_cols).apply(lambda g: pd.Series({\n",
    "        \"mape\": np.sum(np.abs(g[y_true_col] - g[y_pred_col]) * g[quantity_col]) / np.sum(g[y_true_col] * g[quantity_col]),\n",
    "        \"bias\": np.sum((g[y_pred_col] - g[y_true_col]) * g[quantity_col]) / np.sum(g[quantity_col])\n",
    "    }), include_groups=False)\n",
    "    \n",
    "    return grouped.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f52d295",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importance(model, feature_names, title):\n",
    "    \"\"\"Plots feature importance from LightGBM model after preprocessing\"\"\"\n",
    "\n",
    "    # Extract LightGBM feature importance\n",
    "    importance = model.named_steps[\"model\"].feature_importances_\n",
    "\n",
    "    # Ensure feature names align with importance array\n",
    "    if len(feature_names) != len(importance):\n",
    "        print(f\"Warning: Feature name length ({len(feature_names)}) does not match importance length ({len(importance)}). Adjusting...\")\n",
    "        feature_names = feature_names[:len(importance)]  # Trim feature names if mismatch occurs\n",
    "\n",
    "    # Create DataFrame\n",
    "    importance_df = pd.DataFrame({\n",
    "        \"feature\": feature_names,\n",
    "        \"importance\": importance\n",
    "    })\n",
    "\n",
    "    # Sort by importance\n",
    "    importance_df = importance_df.sort_values(by=\"importance\", ascending=False)\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    sns.barplot(data=importance_df, x=\"importance\", y=\"feature\", hue=\"feature\", legend=False, palette=\"coolwarm\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Importance\")\n",
    "    plt.ylabel(\"Feature\")\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef568ad-694f-489c-bd9f-afa6f0b1245d",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6ee47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype_dict = {\n",
    "    'structure_level_4': 'object',\n",
    "    'structure_level_3': 'object',\n",
    "    'structure_level_2': 'object',\n",
    "    'structure_level_1': 'object',\n",
    "    'sku': 'int64',\n",
    "    'competitor': 'object',\n",
    "    'chain_campaign': 'object',\n",
    "    'quantity': 'float64',\n",
    "    'pvp_was': 'float64',\n",
    "    'discount': 'float64',\n",
    "    'flag_promo': 'float64',\n",
    "    'leaflet': 'object',\n",
    "    'pvp_is': 'float64',\n",
    "    'flg_filled_gap': 'float64',\n",
    "    'month': 'int32',\n",
    "    'day_of_week': 'int32',\n",
    "    'week_of_month': 'int64',\n",
    "    'holiday': 'object',\n",
    "    'holiday_importance': 'int64',\n",
    "    'apparent_temperature_mean': 'int64',\n",
    "    'precipitation_sum': 'int64',\n",
    "    'abc': 'object',\n",
    "    'seil': 'object',\n",
    "    'xyz': 'object',\n",
    "    'pvp_is_lag_1': 'float64',\n",
    "    'pvp_is_lag_7': 'float64',\n",
    "    'pvp_is_lag_30': 'float64',\n",
    "    'discount_lag_1': 'float64',\n",
    "    'discount_lag_7': 'float64',\n",
    "    'discount_lag_30': 'float64',\n",
    "    'days_since_last_promo': 'float64',\n",
    "    'rolling_mean_7': 'float64',\n",
    "    'rolling_mean_30': 'float64',\n",
    "    'avg_discount_w_L3': 'float64',\n",
    "    'promo_part_w_L3': 'float64',\n",
    "    'avg_discount_w_sku': 'float64',\n",
    "    'promo_part_w_sku': 'float64',\n",
    "    'discount_chain': 'float64',\n",
    "    'leaflet_chain': 'object'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c878d40-0a17-4290-b824-f5ea7a174f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_input = pd.read_csv(\"prepared_data/data.csv\",dtype=dtype_dict, parse_dates=['date'])\n",
    "df_input['avg_discount_w_L3'] = df_input['avg_discount_w_L3'].round(2)\n",
    "df_input['promo_part_w_L3'] = df_input['promo_part_w_L3'].round(2)\n",
    "df_input['avg_discount_w_sku'] = df_input['avg_discount_w_sku'].round(2)\n",
    "df_input['promo_part_w_sku'] = df_input['promo_part_w_sku'].round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ef6d95-4198-4798-934c-2d85b0076b63",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d882190",
   "metadata": {},
   "source": [
    "### LGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd17c26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_structure_level_1=df_input['structure_level_1'].sort_values().unique()\n",
    "list_competitor=df_input['competitor'].sort_values().unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bb3ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_input[['sku', 'date','competitor', ## keys\n",
    "                'pvp_is', ## target \n",
    "                'pvp_was',\n",
    "                'quantity', ## used for evaluation\n",
    "                ## features:\n",
    "                'structure_level_3', 'structure_level_1', #'structure_level_3', 'structure_level_2', 'structure_level_1', \n",
    "                'holiday_importance','avg_discount_w_L3','promo_part_w_L3','avg_discount_w_sku','promo_part_w_sku',\n",
    "                'month', 'day_of_week', 'week_of_month',\n",
    "                'apparent_temperature_mean', 'precipitation_sum', \n",
    "                'abc', 'seil', 'xyz']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0366c758",
   "metadata": {},
   "outputs": [],
   "source": [
    "####### COM PVP IS MAS SEM LAGS\n",
    "\n",
    "## Train a model for each competitor and structure of level 1\n",
    "all_predictions_lgbm_pvp_is = []\n",
    "for struct1 in list_structure_level_1:\n",
    "    for comp in list_competitor:\n",
    "        print(\"***************************************************************************************************\")\n",
    "        print(\"Structure_level_1: \", struct1)\n",
    "        print(\"Competitor: \", comp)\n",
    "\n",
    "        ## 1. Filter dataset\n",
    "        df_it = df[((df[\"competitor\"] == comp) & (df[\"structure_level_1\"]==struct1))].copy()\n",
    "        df_it = df_it.drop('structure_level_1', axis=1)\n",
    "        print(\"df_it: \", len(df_it))\n",
    "\n",
    "\n",
    "        ## 2. Define train-test split date (e.g., last 4 months for testing)\n",
    "        excluded_cols = [\"sku\", \"date\", \"competitor\", \"pvp_is\", \"quantity\"]\n",
    "        train_cutoff = pd.to_datetime(\"2024-06-04\")  \n",
    "        df_it_train = df_it[df_it[\"date\"] < train_cutoff]\n",
    "        X_train_it = df_it_train.drop(columns=excluded_cols)\n",
    "        y_train_it = df_it_train[\"pvp_is\"]\n",
    "        df_it_test = df_it[df_it[\"date\"] >= train_cutoff]\n",
    "        X_test_it = df_it_test.drop(columns=excluded_cols)\n",
    "        y_test_it = df_it_test[\"pvp_is\"]\n",
    "\n",
    "        ## 3. Apply Transformations\n",
    "        categorical_features = [\"abc\", \"seil\", \"xyz\", 'structure_level_3']\n",
    "        continuous_features = [\"holiday_importance\",\n",
    "                            \"apparent_temperature_mean\", \"precipitation_sum\", \n",
    "                            \"month\", \"day_of_week\", \"week_of_month\", 'avg_discount_w_L3','promo_part_w_L3','avg_discount_w_sku','promo_part_w_sku']\n",
    "\n",
    "        preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_features),\n",
    "                (\"num\", StandardScaler(), continuous_features)\n",
    "            ])\n",
    "\n",
    "        ## 4. Hyperparameter Tunning\n",
    "        param_grid = {\n",
    "            \"model__learning_rate\": [0.01, 0.05, 0.1],\n",
    "            \"model__n_estimators\": [100, 300, 500],\n",
    "            \"model__max_depth\": [3, 5, 7],\n",
    "            \"model__num_leaves\": [20, 31, 40],\n",
    "        }\n",
    "        pipeline_it = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"model\", lgb.LGBMRegressor(random_state=42, verbose=-1))])\n",
    "        random_search_it = RandomizedSearchCV(pipeline_it, param_grid,n_iter=81, cv=3, scoring=\"neg_mean_absolute_error\", n_jobs=-1, random_state=42, verbose=0)\n",
    "        random_search_it.fit(X_train_it, y_train_it)\n",
    "        best_params_it = random_search_it.best_params_\n",
    "        print(\"\\n Best Parameters for \", comp, \" and structure_level_1 \", struct1, \": \\n\",  best_params_it)\n",
    "\n",
    "        ## 5. Evaluate predictions\n",
    "        df_it_input = df_input[((df_input[\"competitor\"] == comp) & (df_input[\"structure_level_1\"]==struct1))].copy()\n",
    "        best_model_it = random_search_it.best_estimator_\n",
    "        y_pred_it = best_model_it.predict(X_test_it)\n",
    "        final_preds_it = df_it_input.loc[X_test_it.index].copy()\n",
    "        final_preds_it[\"pvp_is_pred\"] = y_pred_it\n",
    "        all_predictions_lgbm_pvp_is.append(final_preds_it)\n",
    "\n",
    "        result_1 = grouped_weighted_mape_bias(final_preds_it, [\"structure_level_1\", \"competitor\"], \"pvp_is\", \"pvp_is_pred\", \"quantity\")\n",
    "        print(\"\\n Weighted MAPE & BIAS for \", comp, \" and structure_level_1 \", struct1, \": \\n\", result_1)\n",
    "        result_2 = grouped_weighted_mape_bias(final_preds_it, [\"structure_level_2\", \"competitor\"], \"pvp_is\", \"pvp_is_pred\", \"quantity\")\n",
    "        print(\"\\n Weighted MAPE & BIAS by structure level 2:\\n\", result_2)\n",
    "\n",
    "        ## 6. Feature Importance\n",
    "        X_train_transformed_it = best_model_it.named_steps[\"preprocessor\"].transform(X_train_it)\n",
    "        feature_names_it = best_model_it.named_steps[\"preprocessor\"].get_feature_names_out()\n",
    "\n",
    "        importance_values = best_model_it.named_steps[\"model\"].feature_importances_\n",
    "        feature_importance_df = pd.DataFrame({\n",
    "            \"Feature\": feature_names_it,\n",
    "            \"Importance (%)\": (importance_values / importance_values.sum()) * 100  # Normalize to %\n",
    "        })\n",
    "        feature_importance_df = feature_importance_df.sort_values(by=\"Importance (%)\", ascending=False)\n",
    "        print(\"\\nFeature Importance - \" + comp + \" and structure_level_1: \")\n",
    "        feature_importance_df.to_csv(f\"feature_imp/lgbm_pvpis_woutLags_feature_importance_structL1_{struct1}_{comp}.csv\")\n",
    "\n",
    "        print(feature_importance_df)\n",
    "        plot_feature_importance(best_model_it, feature_names_it, \"Feature Importance - \" + comp + \" and structure_level_1 \" + struct1)\n",
    "\n",
    "\n",
    "        # 7. Retrain model on full dataset\n",
    "        clean_params_it = {key.replace(\"model__\", \"\"): value for key, value in best_params_it.items()}\n",
    "        final_model_it = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"model\", lgb.LGBMRegressor(**clean_params_it))])\n",
    "        X_it = df_it.drop(columns=excluded_cols)\n",
    "        y_it = df_it[\"pvp_is\"]\n",
    "        final_model_it.fit(X_it, y_it)\n",
    "        \n",
    "        ## 8. Save Model\n",
    "        TMP_DIR = 'models'\n",
    "        joblib.dump(final_model_it, os.path.join(TMP_DIR, f'final_model_it_structL1_{struct1}_{comp}.pickle'))\n",
    "        print(\"Model \", f'lgbm_pvpis_woutLags_model_it_structL1_{struct1}_{comp}.pickle',  \" saved in '\", TMP_DIR, \"'\\n\\n\")\n",
    "\n",
    "df_all_predictions_lgbm_pvp_is = pd.concat(all_predictions_lgbm_pvp_is, ignore_index=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
