{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a027e6b5-33a4-4fc4-ab59-e637834d914f",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b531056d-5081-42c3-b657-e4727b85a7d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\catar\\AppData\\Local\\Temp\\ipykernel_18628\\1149668497.py:2: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import ks_2samp\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, precision_recall_curve\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.metrics import mean_absolute_percentage_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290b8fa0",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b070ed40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grouped_weighted_mape_bias(df, group_cols, y_true_col, y_pred_col, quantity_col):\n",
    "    # Function to compute weighted MAPE & BIAS\n",
    "    grouped = df.groupby(group_cols).apply(lambda g: pd.Series({\n",
    "        \"mape\": np.sum(np.abs(g[y_true_col] - g[y_pred_col]) * g[quantity_col]) / np.sum(g[y_true_col] * g[quantity_col]),\n",
    "        \"bias\": np.sum((g[y_pred_col] - g[y_true_col]) * g[quantity_col]) / np.sum(g[quantity_col])\n",
    "    }), include_groups=False)\n",
    "    \n",
    "    return grouped.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f52d295",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importance(model, feature_names, title):\n",
    "    \"\"\"Plots feature importance from LightGBM model after preprocessing\"\"\"\n",
    "\n",
    "    # Extract LightGBM feature importance\n",
    "    importance = model.named_steps[\"model\"].feature_importances_\n",
    "\n",
    "    # Ensure feature names align with importance array\n",
    "    if len(feature_names) != len(importance):\n",
    "        print(f\"Warning: Feature name length ({len(feature_names)}) does not match importance length ({len(importance)}). Adjusting...\")\n",
    "        feature_names = feature_names[:len(importance)]  # Trim feature names if mismatch occurs\n",
    "\n",
    "    # Create DataFrame\n",
    "    importance_df = pd.DataFrame({\n",
    "        \"feature\": feature_names,\n",
    "        \"importance\": importance\n",
    "    })\n",
    "\n",
    "    # Sort by importance\n",
    "    importance_df = importance_df.sort_values(by=\"importance\", ascending=False)\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    sns.barplot(data=importance_df, x=\"importance\", y=\"feature\", hue=\"feature\", legend=False, palette=\"coolwarm\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Importance\")\n",
    "    plt.ylabel(\"Feature\")\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef568ad-694f-489c-bd9f-afa6f0b1245d",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e6ee47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype_dict = {\n",
    "    'structure_level_4': 'object',\n",
    "    'structure_level_3': 'object',\n",
    "    'structure_level_2': 'object',\n",
    "    'structure_level_1': 'object',\n",
    "    'sku': 'int64',\n",
    "    'competitor': 'object',\n",
    "    'chain_campaign': 'object',\n",
    "    'quantity': 'float64',\n",
    "    'pvp_was': 'float64',\n",
    "    'discount': 'float64',\n",
    "    'flag_promo': 'float64',\n",
    "    'leaflet': 'object',\n",
    "    'pvp_is': 'float64',\n",
    "    'flg_filled_gap': 'float64',\n",
    "    'month': 'int32',\n",
    "    'day_of_week': 'int32',\n",
    "    'week_of_month': 'int64',\n",
    "    'holiday': 'object',\n",
    "    'holiday_importance': 'int64',\n",
    "    'apparent_temperature_mean': 'int64',\n",
    "    'precipitation_sum': 'int64',\n",
    "    'abc': 'object',\n",
    "    'seil': 'object',\n",
    "    'xyz': 'object',\n",
    "    'pvp_is_lag_1': 'float64',\n",
    "    'pvp_is_lag_7': 'float64',\n",
    "    'pvp_is_lag_30': 'float64',\n",
    "    'discount_lag_1': 'float64',\n",
    "    'discount_lag_7': 'float64',\n",
    "    'discount_lag_30': 'float64',\n",
    "    'days_since_last_promo': 'float64',\n",
    "    'rolling_mean_7': 'float64',\n",
    "    'rolling_mean_30': 'float64',\n",
    "    'avg_discount_w_L3': 'float64',\n",
    "    'promo_part_w_L3': 'float64',\n",
    "    'avg_discount_w_sku': 'float64',\n",
    "    'promo_part_w_sku': 'float64',\n",
    "    'discount_chain': 'float64',\n",
    "    'leaflet_chain': 'object'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c878d40-0a17-4290-b824-f5ea7a174f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_input = pd.read_csv(\"prepared_data/data.csv\",dtype=dtype_dict, parse_dates=['date'])\n",
    "df_input['avg_discount_w_L3'] = df_input['avg_discount_w_L3'].round(2)\n",
    "df_input['promo_part_w_L3'] = df_input['promo_part_w_L3'].round(2)\n",
    "df_input['avg_discount_w_sku'] = df_input['avg_discount_w_sku'].round(2)\n",
    "df_input['promo_part_w_sku'] = df_input['promo_part_w_sku'].round(2)\n",
    "\n",
    "df = df_input[['sku', 'date','competitor', ## keys\n",
    "         'pvp_is', ## target \n",
    "         'quantity', ## used for evaluation\n",
    "         ## features:\n",
    "         'structure_level_2', #'structure_level_3', 'structure_level_2', 'structure_level_1', \n",
    "         'chain_campaign', 'holiday_importance',\n",
    "         'month', 'day_of_week', 'week_of_month',\n",
    "         'apparent_temperature_mean', 'precipitation_sum', 'avg_discount_w_L3','promo_part_w_L3','avg_discount_w_sku','promo_part_w_sku',\n",
    "         'abc', 'seil', 'xyz', 'days_since_last_promo', 'discount_lag_7', 'pvp_is_lag_7',\n",
    "          'discount_chain', 'leaflet_chain']]\n",
    "\n",
    "#  'pvp_was' 'discount', 'flag_promo', 'leaflet'  flg_filled_gap 'pvp_is_lag_1', 'pvp_is_lag_7', 'pvp_is_lag_30', 'rolling_mean_7' 'rolling_mean_30' holiday"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ef6d95-4198-4798-934c-2d85b0076b63",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad64f034",
   "metadata": {},
   "source": [
    "### Baseline -> consider always the last pvp_was"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6d86276",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add pvp_was_lag\n",
    "df_input[f\"pvp_was_lag_1\"] = df_input.groupby([\"sku\", \"competitor\"])[\"pvp_was\"].shift(1)\n",
    "\n",
    "# Filter data for CompetitorA and CompetitorB\n",
    "df_compA = df_input[df_input[\"competitor\"] == \"competitorA\"].copy()\n",
    "df_compB = df_input[df_input[\"competitor\"] == \"competitorB\"].copy()\n",
    "\n",
    "# Define train-test split date (e.g., last 4 months for testing)\n",
    "train_cutoff = pd.to_datetime(\"2024-06-04\")  \n",
    "\n",
    "df_compA_train = df_compA[df_compA[\"date\"] < train_cutoff]\n",
    "df_compA_test = df_compA[df_compA[\"date\"] >= train_cutoff]\n",
    "df_compB_train = df_compB[df_compB[\"date\"] < train_cutoff]\n",
    "df_compB_test = df_compB[df_compB[\"date\"] >= train_cutoff]\n",
    "\n",
    "df_compA_train = df_compA_train.sort_values(by=['sku', 'date'])\n",
    "df_compB_train = df_compB_train.sort_values(by=['sku', 'date'])\n",
    "\n",
    "# Get the last pvp_was for each SKU (latest date per sku)\n",
    "df_last_pvp_was_A = df_compA_train.groupby('sku').tail(1).reset_index(drop=True)[['sku', 'pvp_was']].rename(columns={\"pvp_was\":\"pvp_is_pred\"})\n",
    "df_last_pvp_was_B = df_compB_train.groupby('sku').tail(1).reset_index(drop=True)[['sku', 'pvp_was']].rename(columns={\"pvp_was\":\"pvp_is_pred\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e95d46e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preds_baseline_A = df_compA.copy()\n",
    "df_preds_baseline_A = (df_preds_baseline_A.merge(df_last_pvp_was_A, on=['sku'], how='left'))\n",
    "\n",
    "df_preds_baseline_B = df_compB.copy()\n",
    "df_preds_baseline_B = (df_preds_baseline_B.merge(df_last_pvp_was_B, on=['sku'], how='left'))\n",
    "\n",
    "# Combine competitor predictions\n",
    "df_preds_baseline = pd.concat([df_preds_baseline_A, df_preds_baseline_B])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56fda325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted MAPE & BIAS by structure:\n",
      "   structure_level_1   competitor      mape      bias\n",
      "0                 1  competitorA  0.131811  1.882359\n",
      "1                 1  competitorB  0.114195  1.492358\n",
      "2                 2  competitorA  0.123169  1.958384\n",
      "3                 2  competitorB  0.116081  0.641929\n",
      "4                 3  competitorA  0.103178  2.183898\n",
      "5                 3  competitorB  0.065196  0.787653\n"
     ]
    }
   ],
   "source": [
    "result_1 = grouped_weighted_mape_bias(df_preds_baseline, [\"structure_level_1\", \"competitor\"], \"pvp_is\", \"pvp_is_pred\", \"quantity\")\n",
    "print(\"Weighted MAPE & BIAS by structure:\\n\", result_1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
