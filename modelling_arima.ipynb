{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a027e6b5-33a4-4fc4-ab59-e637834d914f",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b531056d-5081-42c3-b657-e4727b85a7d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\catar\\AppData\\Local\\Temp\\ipykernel_16648\\1405592005.py:2: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "from scipy.stats import ks_2samp\n",
    "from plotchecker import LinePlotChecker, ScatterPlotChecker, BarPlotChecker\n",
    "import seaborn as sns\n",
    "import requests\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler,OrdinalEncoder\n",
    "from sklearn.metrics import precision_score, recall_score, precision_recall_curve\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from lightgbm import LGBMClassifier, LGBMRegressor \n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from pmdarima import auto_arima"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290b8fa0",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b070ed40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grouped_weighted_mape_bias(df, group_cols, y_true_col, y_pred_col, quantity_col):\n",
    "    # Function to compute weighted MAPE & BIAS\n",
    "    grouped = df.groupby(group_cols).apply(lambda g: pd.Series({\n",
    "        \"mape\": np.sum(np.abs(g[y_true_col] - g[y_pred_col]) * g[quantity_col]) / np.sum(g[y_true_col] * g[quantity_col]),\n",
    "        \"bias\": np.sum((g[y_pred_col] - g[y_true_col]) * g[quantity_col]) / np.sum(g[quantity_col])\n",
    "    }), include_groups=False)\n",
    "    \n",
    "    return grouped.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f52d295",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importance(model, feature_names, title):\n",
    "    \"\"\"Plots feature importance from LightGBM model after preprocessing\"\"\"\n",
    "\n",
    "    # Extract LightGBM feature importance\n",
    "    importance = model.named_steps[\"model\"].feature_importances_\n",
    "\n",
    "    # Ensure feature names align with importance array\n",
    "    if len(feature_names) != len(importance):\n",
    "        print(f\"Warning: Feature name length ({len(feature_names)}) does not match importance length ({len(importance)}). Adjusting...\")\n",
    "        feature_names = feature_names[:len(importance)]  # Trim feature names if mismatch occurs\n",
    "\n",
    "    # Create DataFrame\n",
    "    importance_df = pd.DataFrame({\n",
    "        \"feature\": feature_names,\n",
    "        \"importance\": importance\n",
    "    })\n",
    "\n",
    "    # Sort by importance\n",
    "    importance_df = importance_df.sort_values(by=\"importance\", ascending=False)\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    sns.barplot(data=importance_df, x=\"importance\", y=\"feature\", hue=\"feature\", legend=False, palette=\"coolwarm\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Importance\")\n",
    "    plt.ylabel(\"Feature\")\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef568ad-694f-489c-bd9f-afa6f0b1245d",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e6ee47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype_dict = {\n",
    "    'structure_level_4': 'object',\n",
    "    'structure_level_3': 'object',\n",
    "    'structure_level_2': 'object',\n",
    "    'structure_level_1': 'object',\n",
    "    'sku': 'int64',\n",
    "    'competitor': 'object',\n",
    "    'chain_campaign': 'object',\n",
    "    'quantity': 'float64',\n",
    "    'pvp_was': 'float64',\n",
    "    'discount': 'float64',\n",
    "    'flag_promo': 'float64',\n",
    "    'leaflet': 'object',\n",
    "    'pvp_is': 'float64',\n",
    "    'flg_filled_gap': 'float64',\n",
    "    'month': 'int32',\n",
    "    'day_of_week': 'int32',\n",
    "    'week_of_month': 'int64',\n",
    "    'holiday': 'object',\n",
    "    'holiday_importance': 'int64',\n",
    "    'apparent_temperature_mean': 'int64',\n",
    "    'precipitation_sum': 'int64',\n",
    "    'abc': 'object',\n",
    "    'seil': 'object',\n",
    "    'xyz': 'object',\n",
    "    'pvp_is_lag_1': 'float64',\n",
    "    'pvp_is_lag_7': 'float64',\n",
    "    'pvp_is_lag_30': 'float64',\n",
    "    'discount_lag_1': 'float64',\n",
    "    'discount_lag_7': 'float64',\n",
    "    'discount_lag_30': 'float64',\n",
    "    'days_since_last_promo': 'float64',\n",
    "    'rolling_mean_7': 'float64',\n",
    "    'rolling_mean_30': 'float64',\n",
    "    'avg_discount_w_L3': 'float64',\n",
    "    'promo_part_w_L3': 'float64',\n",
    "    'avg_discount_w_sku': 'float64',\n",
    "    'promo_part_w_sku': 'float64',\n",
    "    'discount_chain': 'float64',\n",
    "    'leaflet_chain': 'object'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c878d40-0a17-4290-b824-f5ea7a174f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_input = pd.read_csv(\"prepared_data/data.csv\",dtype=dtype_dict, parse_dates=['date'])\n",
    "df_input['avg_discount_w_L3'] = df_input['avg_discount_w_L3'].round(2)\n",
    "df_input['promo_part_w_L3'] = df_input['promo_part_w_L3'].round(2)\n",
    "df_input['avg_discount_w_sku'] = df_input['avg_discount_w_sku'].round(2)\n",
    "df_input['promo_part_w_sku'] = df_input['promo_part_w_sku'].round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ef6d95-4198-4798-934c-2d85b0076b63",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d290f1-1e69-41b4-af34-1ebc1e45767b",
   "metadata": {},
   "source": [
    "### ARIMA / SARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fdbf59d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_structure_level_1=df_input['structure_level_1'].sort_values().unique()\n",
    "list_competitor=df_input['competitor'].sort_values().unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "03e55caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_input[['sku', 'date','competitor', ## keys\n",
    "                'pvp_is', ## target \n",
    "                'quantity', ## used for evaluation\n",
    "                ## features:\n",
    "                'structure_level_3', 'structure_level_1', #'structure_level_3', 'structure_level_2', 'structure_level_1', \n",
    "                'chain_campaign', 'holiday_importance','avg_discount_w_L3','promo_part_w_L3',\n",
    "                'month', 'day_of_week', 'week_of_month',\n",
    "                'apparent_temperature_mean', 'precipitation_sum', \n",
    "                'abc', 'seil', 'xyz',\n",
    "                'discount_chain', 'leaflet_chain']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1169eb7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\catar\\AppData\\Local\\Temp\\ipykernel_18420\\2830822718.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"date\"] = pd.to_datetime(df[\"date\"])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************************************************************************************************\n",
      "Structure_level_1: 1, Competitor: competitorB\n",
      "sku  2821  and  competitorB  too short <30:  0\n",
      "sku  2734  and  competitorB  too short <30:  0\n",
      "sku  2786  and  competitorB  too short <30:  0\n",
      "sku  2754  and  competitorB  too short <30:  0\n",
      "sku  2664  and  competitorB  too short <30:  0\n",
      "sku  2992  and  competitorB  too short <30:  0\n",
      "sku  2993  and  competitorB  too short <30:  0\n",
      "sku  2822  and  competitorB  too short <30:  0\n",
      "sku  2699  and  competitorB  too short <30:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\catar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pmdarima\\arima\\_auto_solvers.py:524: ModelFitWarning: Error fitting  ARIMA(2,0,1)(2,1,0)[7] intercept (if you do not want to see these warnings, run with error_action=\"ignore\").\n",
      "Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\catar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pmdarima\\arima\\_auto_solvers.py\", line 508, in _fit_candidate_model\n",
      "    fit.fit(y, X=X, **fit_params)\n",
      "  File \"c:\\Users\\catar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pmdarima\\arima\\arima.py\", line 603, in fit\n",
      "    self._fit(y, X, **fit_args)\n",
      "  File \"c:\\Users\\catar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pmdarima\\arima\\arima.py\", line 524, in _fit\n",
      "    fit, self.arima_res_ = _fit_wrapper()\n",
      "  File \"c:\\Users\\catar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pmdarima\\arima\\arima.py\", line 510, in _fit_wrapper\n",
      "    fitted = arima.fit(\n",
      "  File \"c:\\Users\\catar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\statsmodels\\tsa\\statespace\\mlemodel.py\", line 705, in fit\n",
      "    mlefit = super().fit(start_params, method=method,\n",
      "  File \"c:\\Users\\catar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\statsmodels\\base\\model.py\", line 566, in fit\n",
      "    xopt, retvals, optim_settings = optimizer._fit(f, score, start_params,\n",
      "  File \"c:\\Users\\catar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\statsmodels\\base\\optimizer.py\", line 243, in _fit\n",
      "    xopt, retvals = func(objective, gradient, start_params, fargs, kwargs,\n",
      "  File \"c:\\Users\\catar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\statsmodels\\base\\optimizer.py\", line 660, in _fit_lbfgs\n",
      "    retvals = optimize.fmin_l_bfgs_b(func, start_params, maxiter=maxiter,\n",
      "  File \"c:\\Users\\catar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_lbfgsb_py.py\", line 277, in fmin_l_bfgs_b\n",
      "    res = _minimize_lbfgsb(fun, x0, args=args, jac=jac, bounds=bounds,\n",
      "  File \"c:\\Users\\catar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_lbfgsb_py.py\", line 441, in _minimize_lbfgsb\n",
      "    f, g = func_and_grad(x)\n",
      "  File \"c:\\Users\\catar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py\", line 345, in fun_and_grad\n",
      "    self._update_grad()\n",
      "  File \"c:\\Users\\catar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py\", line 307, in _update_grad\n",
      "    self.g = self._wrapped_grad(self.x, f0=self.f)\n",
      "  File \"c:\\Users\\catar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py\", line 48, in wrapped1\n",
      "    return approx_derivative(\n",
      "  File \"c:\\Users\\catar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_numdiff.py\", line 523, in approx_derivative\n",
      "    return _dense_difference(fun_wrapped, x0, f0, h,\n",
      "  File \"c:\\Users\\catar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_numdiff.py\", line 596, in _dense_difference\n",
      "    df = fun(x1) - f0\n",
      "  File \"c:\\Users\\catar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_numdiff.py\", line 474, in fun_wrapped\n",
      "    f = np.atleast_1d(fun(x, *args, **kwargs))\n",
      "  File \"c:\\Users\\catar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py\", line 21, in wrapped\n",
      "    fx = fun(np.copy(x), *args)\n",
      "  File \"c:\\Users\\catar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\statsmodels\\base\\model.py\", line 534, in f\n",
      "    return -self.loglike(params, *args) / nobs\n",
      "  File \"c:\\Users\\catar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\statsmodels\\tsa\\statespace\\mlemodel.py\", line 940, in loglike\n",
      "    loglike = self.ssm.loglike(complex_step=complex_step, **kwargs)\n",
      "  File \"c:\\Users\\catar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\statsmodels\\tsa\\statespace\\kalman_filter.py\", line 1001, in loglike\n",
      "    kfilter = self._filter(**kwargs)\n",
      "  File \"c:\\Users\\catar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\statsmodels\\tsa\\statespace\\kalman_filter.py\", line 921, in _filter\n",
      "    self._initialize_state(prefix=prefix, complex_step=complex_step)\n",
      "  File \"c:\\Users\\catar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\statsmodels\\tsa\\statespace\\representation.py\", line 1058, in _initialize_state\n",
      "    self._statespaces[prefix].initialize(self.initialization,\n",
      "  File \"statsmodels\\\\tsa\\\\statespace\\\\_representation.pyx\", line 1373, in statsmodels.tsa.statespace._representation.dStatespace.initialize\n",
      "  File \"statsmodels\\\\tsa\\\\statespace\\\\_representation.pyx\", line 1362, in statsmodels.tsa.statespace._representation.dStatespace.initialize\n",
      "  File \"statsmodels\\\\tsa\\\\statespace\\\\_initialization.pyx\", line 288, in statsmodels.tsa.statespace._initialization.dInitialization.initialize\n",
      "  File \"statsmodels\\\\tsa\\\\statespace\\\\_initialization.pyx\", line 406, in statsmodels.tsa.statespace._initialization.dInitialization.initialize_stationary_stationary_cov\n",
      "  File \"statsmodels\\\\tsa\\\\statespace\\\\_tools.pyx\", line 1548, in statsmodels.tsa.statespace._tools._dsolve_discrete_lyapunov\n",
      "numpy.linalg.LinAlgError: LU decomposition error.\n",
      "\n",
      "  warnings.warn(warning_str, ModelFitWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sku  2712  and  competitorB  too short <30:  0\n",
      "sku  2669  and  competitorB  too short <30:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\catar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pmdarima\\arima\\auto.py:444: UserWarning: Input time-series is completely constant; returning a (0, 0, 0) ARMA.\n",
      "  warnings.warn('Input time-series is completely constant; '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sku  2672  and  competitorB  too short <30:  0\n",
      "sku  2673  and  competitorB  too short <30:  0\n",
      "sku  2779  and  competitorB  too short <30:  0\n",
      "sku  2881  and  competitorB  too short <30:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\catar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pmdarima\\arima\\_auto_solvers.py:524: ModelFitWarning: Error fitting  ARIMA(3,0,2)(2,1,0)[7] intercept (if you do not want to see these warnings, run with error_action=\"ignore\").\n",
      "Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\catar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pmdarima\\arima\\_auto_solvers.py\", line 508, in _fit_candidate_model\n",
      "    fit.fit(y, X=X, **fit_params)\n",
      "  File \"c:\\Users\\catar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pmdarima\\arima\\arima.py\", line 603, in fit\n",
      "    self._fit(y, X, **fit_args)\n",
      "  File \"c:\\Users\\catar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pmdarima\\arima\\arima.py\", line 524, in _fit\n",
      "    fit, self.arima_res_ = _fit_wrapper()\n",
      "  File \"c:\\Users\\catar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pmdarima\\arima\\arima.py\", line 510, in _fit_wrapper\n",
      "    fitted = arima.fit(\n",
      "  File \"c:\\Users\\catar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\statsmodels\\tsa\\statespace\\mlemodel.py\", line 705, in fit\n",
      "    mlefit = super().fit(start_params, method=method,\n",
      "  File \"c:\\Users\\catar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\statsmodels\\base\\model.py\", line 566, in fit\n",
      "    xopt, retvals, optim_settings = optimizer._fit(f, score, start_params,\n",
      "  File \"c:\\Users\\catar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\statsmodels\\base\\optimizer.py\", line 243, in _fit\n",
      "    xopt, retvals = func(objective, gradient, start_params, fargs, kwargs,\n",
      "  File \"c:\\Users\\catar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\statsmodels\\base\\optimizer.py\", line 660, in _fit_lbfgs\n",
      "    retvals = optimize.fmin_l_bfgs_b(func, start_params, maxiter=maxiter,\n",
      "  File \"c:\\Users\\catar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_lbfgsb_py.py\", line 277, in fmin_l_bfgs_b\n",
      "    res = _minimize_lbfgsb(fun, x0, args=args, jac=jac, bounds=bounds,\n",
      "  File \"c:\\Users\\catar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_lbfgsb_py.py\", line 441, in _minimize_lbfgsb\n",
      "    f, g = func_and_grad(x)\n",
      "  File \"c:\\Users\\catar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py\", line 344, in fun_and_grad\n",
      "    self._update_fun()\n",
      "  File \"c:\\Users\\catar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py\", line 295, in _update_fun\n",
      "    fx = self._wrapped_fun(self.x)\n",
      "  File \"c:\\Users\\catar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py\", line 21, in wrapped\n",
      "    fx = fun(np.copy(x), *args)\n",
      "  File \"c:\\Users\\catar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\statsmodels\\base\\model.py\", line 534, in f\n",
      "    return -self.loglike(params, *args) / nobs\n",
      "  File \"c:\\Users\\catar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\statsmodels\\tsa\\statespace\\mlemodel.py\", line 940, in loglike\n",
      "    loglike = self.ssm.loglike(complex_step=complex_step, **kwargs)\n",
      "  File \"c:\\Users\\catar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\statsmodels\\tsa\\statespace\\kalman_filter.py\", line 1001, in loglike\n",
      "    kfilter = self._filter(**kwargs)\n",
      "  File \"c:\\Users\\catar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\statsmodels\\tsa\\statespace\\kalman_filter.py\", line 921, in _filter\n",
      "    self._initialize_state(prefix=prefix, complex_step=complex_step)\n",
      "  File \"c:\\Users\\catar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\statsmodels\\tsa\\statespace\\representation.py\", line 1058, in _initialize_state\n",
      "    self._statespaces[prefix].initialize(self.initialization,\n",
      "  File \"statsmodels\\\\tsa\\\\statespace\\\\_representation.pyx\", line 1373, in statsmodels.tsa.statespace._representation.dStatespace.initialize\n",
      "  File \"statsmodels\\\\tsa\\\\statespace\\\\_representation.pyx\", line 1362, in statsmodels.tsa.statespace._representation.dStatespace.initialize\n",
      "  File \"statsmodels\\\\tsa\\\\statespace\\\\_initialization.pyx\", line 288, in statsmodels.tsa.statespace._initialization.dInitialization.initialize\n",
      "  File \"statsmodels\\\\tsa\\\\statespace\\\\_initialization.pyx\", line 406, in statsmodels.tsa.statespace._initialization.dInitialization.initialize_stationary_stationary_cov\n",
      "  File \"statsmodels\\\\tsa\\\\statespace\\\\_tools.pyx\", line 1548, in statsmodels.tsa.statespace._tools._dsolve_discrete_lyapunov\n",
      "numpy.linalg.LinAlgError: LU decomposition error.\n",
      "\n",
      "  warnings.warn(warning_str, ModelFitWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sku  2628  and  competitorB  too short <30:  0\n",
      "sku  2941  and  competitorB  too short <30:  0\n",
      "sku  2939  and  competitorB  too short <30:  0\n",
      "sku  2940  and  competitorB  too short <30:  0\n",
      "sku  2942  and  competitorB  too short <30:  0\n",
      "sku  2623  and  competitorB  too short <30:  0\n",
      "sku  2620  and  competitorB  too short <30:  0\n",
      "sku  2605  and  competitorB  too short <30:  0\n",
      "sku  2602  and  competitorB  too short <30:  0\n",
      "\n",
      "=== Summary by Structure & Competitor ===\n",
      "  structure_level_1   competitor      mape     bias\n",
      "0                 1  competitorB  0.060523 -1.20181\n",
      "\n",
      " Models and predictions saved.\n"
     ]
    }
   ],
   "source": [
    "# Ensure the models directory exists\n",
    "TMP_DIR = \"models_sarima\"\n",
    "os.makedirs(TMP_DIR, exist_ok=True)\n",
    "\n",
    "# Ensure date is in datetime format\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "df = df.sort_values(\"date\")\n",
    "train_cutoff = pd.to_datetime(\"2024-06-04\")\n",
    "\n",
    "results = []\n",
    "all_predictions = []\n",
    "\n",
    "# Loop through structure_level_1 & competitors\n",
    "for struct1 in list_structure_level_1:\n",
    "    for comp in list_competitor:\n",
    "        print(\"***************************************************************************************************\")\n",
    "        print(f\"Structure_level_1: {struct1}, Competitor: {comp}\")\n",
    "\n",
    "        ## 1️. Filter dataset\n",
    "        df_filtered = df[(df['structure_level_1'] == struct1) & (df['competitor'] == comp)].copy()\n",
    "\n",
    "        for sku in df_filtered['sku'].unique():\n",
    "            df_sku = df_filtered[df_filtered['sku'] == sku].sort_values('date')\n",
    "\n",
    "            if len(df_sku) < 30:\n",
    "                print(\"sku \", sku, \" and \", comp, \" too short <30: \", len(df_sku))\n",
    "                continue  # skip too short series\n",
    "\n",
    "            df_sku = df_sku[['date', 'pvp_is', 'quantity']].dropna().drop_duplicates()\n",
    "            df_sku['date'] = pd.to_datetime(df_sku['date'])\n",
    "            df_sku = df_sku.set_index('date')\n",
    "            df_sku = df_sku.asfreq('D')\n",
    "            df_sku = df_sku.ffill() ## there should be no nan at this step -> just doing to prevent\n",
    "\n",
    "            # Train-test split\n",
    "            df_train = df_sku[df_sku.index < train_cutoff].dropna()\n",
    "            df_test = df_sku[df_sku.index >= train_cutoff].dropna()\n",
    "\n",
    "            if len(df_test) == 0 or len(df_train) < 14:\n",
    "                print(\"sku \", sku, \" and \", comp, \" too short len(df_test)==0: \", len(df_test), \" or len(df_train)<14: \",len(df_train))\n",
    "                continue\n",
    "            try:\n",
    "                # Fit auto-SARIMA model\n",
    "                model = auto_arima(\n",
    "                    df_train['pvp_is'],\n",
    "                    start_p=1, start_q=1, max_p=3, max_q=3,\n",
    "                    d=None,           # Let it infer if differencing is needed\n",
    "                    seasonal=True,\n",
    "                    m=7,              # Weekly pattern\n",
    "                    start_P=0, start_Q=0, max_P=2, max_Q=2,\n",
    "                    D=1,\n",
    "                    suppress_warnings=True,\n",
    "                    stepwise=True\n",
    "                )\n",
    "\n",
    "                forecast = model.predict(n_periods=len(df_test))\n",
    "                if np.all(np.isnan(forecast)):\n",
    "                    print(f\"Forecast returned all NaNs for SKU {sku}\")\n",
    "                    continue\n",
    "\n",
    "                # Evaluate\n",
    "                df_test = df_test.copy()\n",
    "                df_test['pvp_is_pred'] = forecast\n",
    "                df_test['sku'] = sku\n",
    "                df_test['competitor'] = comp\n",
    "                df_test['structure_level_1'] = struct1\n",
    "                df_test = df_test.reset_index().rename(columns={'index': 'date'})\n",
    "\n",
    "                all_predictions.append(df_test)\n",
    "\n",
    "                df_test['abs_error'] = np.abs(df_test['pvp_is_pred'] - df_test['pvp_is'])\n",
    "                df_test['error'] = df_test['pvp_is_pred'] - df_test['pvp_is']\n",
    "\n",
    "                # Use quantity for weighting\n",
    "                df_test['weight'] = df_test['quantity'].fillna(0)\n",
    "\n",
    "                total_weight = df_test['weight'].sum()\n",
    "                if total_weight == 0:\n",
    "                    mape = np.nan\n",
    "                    bias = np.nan\n",
    "                else:\n",
    "                    mape = (df_test['abs_error'] / df_test['pvp_is'] * df_test['weight']).sum() / total_weight\n",
    "                    bias = (df_test['error'] * df_test['weight']).sum() / total_weight\n",
    "\n",
    "                results.append({\n",
    "                    'structure_level_1': struct1,\n",
    "                    'competitor': comp,\n",
    "                    'sku': sku,\n",
    "                    'mape': mape,\n",
    "                    'bias': bias,\n",
    "                    'model_params': model.get_params()\n",
    "                })\n",
    "\n",
    "                # Retrain on full data\n",
    "                model.fit(df_sku['pvp_is'])\n",
    "                model_path = os.path.join(TMP_DIR, f\"sarima_struct{struct1}_{comp}_sku{sku}.pkl\")\n",
    "                joblib.dump(model, model_path)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\" Failed for SKU {sku}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if len(results)>0:\n",
    "            # Combine and summarize results\n",
    "            df_predictions = pd.concat(all_predictions, ignore_index=True)\n",
    "            df_sarima_results = pd.DataFrame(results)\n",
    "            \n",
    "    print(\"\\n=== Summary by Structure & Competitor ===\")\n",
    "    print(grouped_weighted_mape_bias(df_predictions, ['structure_level_1', 'competitor'], 'pvp_is', 'pvp_is_pred', 'quantity'))\n",
    "\n",
    "    print(\"\\n Models and predictions saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "26b0765c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_predictions.to_csv(\"20250512_df_predictions_auto_arima_57_skus_compA_structL1_1.csv\")\n",
    "df_predictions.to_csv(\"20250512_df_predictions_auto_arima_57_skus_compB_structL1_1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ce13eac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Summary by Structure & Competitor ===\n",
      "  structure_level_1   competitor      mape      bias\n",
      "0                 1  competitorA  0.119936 -2.381066\n",
      "\n",
      " nº skus executed:  57\n",
      "\n",
      " Models and predictions saved.\n"
     ]
    }
   ],
   "source": [
    "if len(results)>0:\n",
    "        # Combine and summarize results\n",
    "        df_predictions = pd.concat(all_predictions, ignore_index=True)\n",
    "        df_sarima_results = pd.DataFrame(results)\n",
    "        \n",
    "print(\"\\n=== Summary by Structure & Competitor ===\")\n",
    "print(grouped_weighted_mape_bias(df_predictions, ['structure_level_1', 'competitor'], 'pvp_is', 'pvp_is_pred', 'quantity'))\n",
    "print(\"\\n nº skus executed: \", df_predictions['sku'].drop_duplicates().count())\n",
    "\n",
    "print(\"\\n Models and predictions saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
